{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "RL - PPO agent .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIneeSEbez3T"
      },
      "source": [
        "# Proximal Policy Optimization Agent Using PyTorch\n",
        "This notebook contains an implementation of PPO algorithm using PyTorch\n",
        "\n",
        "The algorithm is then tested using OpenAI's cartpole environment\n",
        "\n",
        "This notebook is based on this great [repo](https://github.com/philtabor/Youtube-Code-Repository/tree/master/ReinforcementLearning/PolicyGradient/PPO) and this great [notebook](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/reinforcement_learning/actor_critic.ipynb) \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YC_EfKRk6M7"
      },
      "source": [
        "### Managing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXdxajJFU7Kw"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions.categorical import Categorical"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "658csY60TJqE",
        "outputId": "e7bec57c-3954-4f1b-c8e2-d9426968a95c"
      },
      "source": [
        "%%bash\n",
        "\n",
        "# install required system dependencies\n",
        "apt-get install -y xvfb x11-utils\n",
        "\n",
        "# install required python dependencies (might need to install additional gym extras depending)\n",
        "pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "Requirement already satisfied: gym[box2d]==0.17.* in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyvirtualdisplay==0.2.* in /usr/local/lib/python3.7/dist-packages (0.2.5)\n",
            "Requirement already satisfied: PyOpenGL==3.1.* in /usr/local/lib/python3.7/dist-packages (3.1.5)\n",
            "Requirement already satisfied: PyOpenGL-accelerate==3.1.* in /usr/local/lib/python3.7/dist-packages (3.1.5)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.3.0)\n",
            "Requirement already satisfied: box2d-py~=2.3.5; extra == \"box2d\" in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (2.3.8)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.7/dist-packages (from pyvirtualdisplay==0.2.*) (0.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]==0.17.*) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s58DPUL7U7K6"
      },
      "source": [
        "class PPOMemory:\n",
        "    def __init__(self, batch_size):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.log_probs = []\n",
        "        self.values = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "    \n",
        "    def generate_batches (self):\n",
        "        n_states = len(self.states)\n",
        "        indices = np.arange(0, n_states)\n",
        "        batch_start = np.arange(n_states, dtype=np.int64)\n",
        "        np.random.shuffle(indices)\n",
        "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
        "        \n",
        "        return np.array(self.states),\\\n",
        "               np.array(self.actions),\\\n",
        "               np.array(self.log_probs),\\\n",
        "               np.array(self.values),\\\n",
        "               np.array(self.rewards),\\\n",
        "               np.array(self.dones),\\\n",
        "               batches\n",
        "    def store_memory(self, state, action, log_prob, value, reward, done):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.log_probs.append(log_prob)\n",
        "        self.values.append(value)\n",
        "        self.rewards.append(reward)\n",
        "        self.dones.append(done)\n",
        "        \n",
        "    def clear_memory(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.log_probs = []\n",
        "        self.values = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NpbuqJeU7K7"
      },
      "source": [
        "class ActorNetwork(nn.Module):\n",
        "    def __init__(self, n_actions, input_dim, alpha,\n",
        "                fc1_dim=256, fc2_dim=256, check_dir='tmp/ppo'):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "        self.checkpoint_file = os.path.join(check_dir, 'actor_ppo')\n",
        "        \n",
        "        self.actor = nn.Sequential(\n",
        "        nn.Linear(*input_dim, fc1_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(fc1_dim, fc2_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(fc2_dim, n_actions),\n",
        "        nn.Softmax(dim=-1)\n",
        "        )\n",
        "        \n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "        \n",
        "    def forward(self, state):\n",
        "        dist = self.actor(state)\n",
        "        dist = Categorical(dist)\n",
        "        \n",
        "        return dist\n",
        "    \n",
        "    def save_checkpoint(self):\n",
        "        T.save(self.state_dict(), self.checkpoint_file)\n",
        "        \n",
        "    def load_checkpoint(self):\n",
        "        self.load_state_dict(T.load(self.checkpoint_file))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--nTtwhaU7K8"
      },
      "source": [
        "class CriticNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, alpha,\n",
        "                fc1_dim=256, fc2_dim=256, check_dir='tmp/ppo'):\n",
        "        super(CriticNetwork, self).__init__()\n",
        "        self.checkpoint_file = os.path.join(check_dir, 'critic_ppo')\n",
        "        self.critic = nn.Sequential(\n",
        "        nn.Linear(*input_dim, fc1_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(fc1_dim, fc2_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(fc2_dim, 1)\n",
        "        )\n",
        "        \n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "        \n",
        "    def forward(self, state):\n",
        "        value = self.critic(state)\n",
        "        \n",
        "        return value\n",
        "    \n",
        "    def save_checkpoint(self):\n",
        "        T.save(self.state_dict(), self.checkpoint_file)\n",
        "        \n",
        "    def load_checkpoint(self):\n",
        "        self.load_state_dict(T.load(self.checkpoint_file))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVMPsMFcU7K8"
      },
      "source": [
        "class PPOAgent:\n",
        "    def __init__(self, n_actions, input_dim, gamma=0.99, lamda=0.95, alpha=0.0003,\n",
        "                clip=0.2, batch_size=64, n_epochs=10):\n",
        "        self.gamma=gamma\n",
        "        self.lamda=lamda\n",
        "        self.clip=clip\n",
        "        self.n_epochs=n_epochs\n",
        "        \n",
        "        self.actor = ActorNetwork(n_actions, input_dim, alpha)\n",
        "        self.critic = CriticNetwork(input_dim, alpha)\n",
        "        self.memory = PPOMemory(batch_size)\n",
        "        \n",
        "    def remember(self, state, action, log_prob, value, reward, done):\n",
        "        self.memory.store_memory(state, action, log_prob, value, reward, done)\n",
        "\n",
        "    def save_models(self):\n",
        "        print('-----Saving Models-----')\n",
        "        self.actor.save_checkpoint()\n",
        "        self.critic.save_checkpoint()\n",
        "\n",
        "    def load_models(self):\n",
        "        print('-----Loading Models-----')\n",
        "        self.actor.load_checkpoint()\n",
        "        self.critic.load_checkpoint()\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
        "\n",
        "        dist = self.actor(state)\n",
        "        action = dist.sample()\n",
        "        value = self.critic(state)\n",
        "\n",
        "        log_prob = T.squeeze(dist.log_prob(action)).item()\n",
        "        action = T.squeeze(action).item()\n",
        "        value = T.squeeze(value).item()\n",
        "\n",
        "        return log_prob, action, value\n",
        "\n",
        "    def learn(self):\n",
        "        for _ in range(self.n_epochs):\n",
        "            states_arr, actions_arr, old_probs_arr, values, rewards,\\\n",
        "            dones, batches = self.memory.generate_batches()\n",
        "\n",
        "            advantages = np.zeros_like(rewards, dtype=np.float32)\n",
        "            for t in range(len(rewards)-1):\n",
        "                discount = 1\n",
        "                a_t = 0\n",
        "                for s in range(t, len(rewards)-1):\n",
        "                    a_t += discount*(rewards[s] + self.gamma*values[s+1]*\\\n",
        "                        (1-int(dones[s])) - values[s])\n",
        "                    discount *= self.gamma * self.lamda\n",
        "                advantages[t] = a_t\n",
        "            advantages = T.tensor(advantages).to(self.actor.device)\n",
        "            values = T.tensor(values).to(self.actor.device)\n",
        "\n",
        "            for batch in batches:\n",
        "                states = T.tensor(states_arr[batch], dtype=T.float).to(self.actor.device)\n",
        "                actions = T.tensor(actions_arr[batch]).to(self.actor.device)\n",
        "                old_probs = T.tensor(old_probs_arr[batch]).to(self.actor.device)\n",
        "\n",
        "                dist = self.actor(states)\n",
        "                critic_values = self.critic(states)\n",
        "                critic_values = T.squeeze(critic_values)\n",
        "\n",
        "                new_probs = dist.log_prob(actions)\n",
        "                prob_ratio = new_probs.exp()/old_probs.exp()\n",
        "                wighted_probs = prob_ratio*advantages[batch]\n",
        "                clipped_wighted_probs = T.clamp(prob_ratio, 1-self.clip, 1+self.clip) * advantages[batch]\n",
        "                actor_loss = -T.min(wighted_probs, clipped_wighted_probs).mean()\n",
        "\n",
        "                critic_loss = (((advantages[batch]+values[batch]) - critic_values)**2).mean()\n",
        "\n",
        "                total_loss = actor_loss + 0.5*critic_loss\n",
        "\n",
        "                self.actor.optimizer.zero_grad()\n",
        "                self.critic.optimizer.zero_grad()\n",
        "                total_loss.backward()\n",
        "                self.actor.optimizer.step()\n",
        "                self.critic.optimizer.step()\n",
        "\n",
        "        self.memory.clear_memory()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-SQ3p5PU7K-",
        "outputId": "02bd1c6a-3438-499c-e680-cc2587b7dd49"
      },
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "N=20\n",
        "batch_size = 5\n",
        "n_epochs = 4\n",
        "clip = 0.15\n",
        "n_games = 300\n",
        "\n",
        "agent = PPOAgent(n_actions=env.action_space.n, \n",
        "                 input_dim=env.observation_space.shape,\n",
        "                batch_size = batch_size, n_epochs=n_epochs, clip=clip)\n",
        "\n",
        "learn_iters = 0\n",
        "best_score = env.reward_range[0]\n",
        "score_history = []\n",
        "n_steps = 0\n",
        "avg_score = 0\n",
        "for _ in range(n_games):\n",
        "    observation = env.reset()\n",
        "    done = False\n",
        "    score = 0\n",
        "    while not done:\n",
        "        prob, action, value = agent.choose_action(observation)\n",
        "        observation_, reward, done, info = env.step(action)\n",
        "        n_steps += 1\n",
        "        score += reward\n",
        "        agent.remember(observation, action, prob, value, reward, done)\n",
        "        if n_steps % N == 0:\n",
        "            agent.learn()\n",
        "            learn_iters+=1\n",
        "        observation = observation_\n",
        "            \n",
        "    score_history.append(score)\n",
        "    avg_score = np.mean(score_history[-100:])\n",
        "    if avg_score > best_score:\n",
        "      best_score = avg_score\n",
        "      agent.save_models()\n",
        "    \n",
        "    print('episode', _, 'score %.1f' % score, 'avg score %.1f' % avg_score,\n",
        "        'time_steps', n_steps, 'learning_steps', learn_iters)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----Saving Models-----\n",
            "episode 0 score 14.0 avg score 14.0 time_steps 14 learning_steps 0\n",
            "-----Saving Models-----\n",
            "episode 1 score 16.0 avg score 15.0 time_steps 30 learning_steps 1\n",
            "-----Saving Models-----\n",
            "episode 2 score 16.0 avg score 15.3 time_steps 46 learning_steps 2\n",
            "-----Saving Models-----\n",
            "episode 3 score 20.0 avg score 16.5 time_steps 66 learning_steps 3\n",
            "-----Saving Models-----\n",
            "episode 4 score 19.0 avg score 17.0 time_steps 85 learning_steps 4\n",
            "episode 5 score 15.0 avg score 16.7 time_steps 100 learning_steps 5\n",
            "episode 6 score 15.0 avg score 16.4 time_steps 115 learning_steps 5\n",
            "episode 7 score 13.0 avg score 16.0 time_steps 128 learning_steps 6\n",
            "episode 8 score 17.0 avg score 16.1 time_steps 145 learning_steps 7\n",
            "episode 9 score 15.0 avg score 16.0 time_steps 160 learning_steps 8\n",
            "episode 10 score 11.0 avg score 15.5 time_steps 171 learning_steps 8\n",
            "-----Saving Models-----\n",
            "episode 11 score 55.0 avg score 18.8 time_steps 226 learning_steps 11\n",
            "-----Saving Models-----\n",
            "episode 12 score 21.0 avg score 19.0 time_steps 247 learning_steps 12\n",
            "episode 13 score 17.0 avg score 18.9 time_steps 264 learning_steps 13\n",
            "episode 14 score 15.0 avg score 18.6 time_steps 279 learning_steps 13\n",
            "-----Saving Models-----\n",
            "episode 15 score 28.0 avg score 19.2 time_steps 307 learning_steps 15\n",
            "episode 16 score 16.0 avg score 19.0 time_steps 323 learning_steps 16\n",
            "-----Saving Models-----\n",
            "episode 17 score 25.0 avg score 19.3 time_steps 348 learning_steps 17\n",
            "-----Saving Models-----\n",
            "episode 18 score 58.0 avg score 21.4 time_steps 406 learning_steps 20\n",
            "-----Saving Models-----\n",
            "episode 19 score 135.0 avg score 27.1 time_steps 541 learning_steps 27\n",
            "-----Saving Models-----\n",
            "episode 20 score 98.0 avg score 30.4 time_steps 639 learning_steps 31\n",
            "-----Saving Models-----\n",
            "episode 21 score 57.0 avg score 31.6 time_steps 696 learning_steps 34\n",
            "-----Saving Models-----\n",
            "episode 22 score 56.0 avg score 32.7 time_steps 752 learning_steps 37\n",
            "-----Saving Models-----\n",
            "episode 23 score 113.0 avg score 36.0 time_steps 865 learning_steps 43\n",
            "-----Saving Models-----\n",
            "episode 24 score 50.0 avg score 36.6 time_steps 915 learning_steps 45\n",
            "-----Saving Models-----\n",
            "episode 25 score 70.0 avg score 37.9 time_steps 985 learning_steps 49\n",
            "-----Saving Models-----\n",
            "episode 26 score 72.0 avg score 39.1 time_steps 1057 learning_steps 52\n",
            "episode 27 score 33.0 avg score 38.9 time_steps 1090 learning_steps 54\n",
            "-----Saving Models-----\n",
            "episode 28 score 71.0 avg score 40.0 time_steps 1161 learning_steps 58\n",
            "-----Saving Models-----\n",
            "episode 29 score 45.0 avg score 40.2 time_steps 1206 learning_steps 60\n",
            "-----Saving Models-----\n",
            "episode 30 score 81.0 avg score 41.5 time_steps 1287 learning_steps 64\n",
            "-----Saving Models-----\n",
            "episode 31 score 67.0 avg score 42.3 time_steps 1354 learning_steps 67\n",
            "-----Saving Models-----\n",
            "episode 32 score 64.0 avg score 43.0 time_steps 1418 learning_steps 70\n",
            "-----Saving Models-----\n",
            "episode 33 score 63.0 avg score 43.6 time_steps 1481 learning_steps 74\n",
            "-----Saving Models-----\n",
            "episode 34 score 45.0 avg score 43.6 time_steps 1526 learning_steps 76\n",
            "-----Saving Models-----\n",
            "episode 35 score 72.0 avg score 44.4 time_steps 1598 learning_steps 79\n",
            "episode 36 score 29.0 avg score 44.0 time_steps 1627 learning_steps 81\n",
            "-----Saving Models-----\n",
            "episode 37 score 155.0 avg score 46.9 time_steps 1782 learning_steps 89\n",
            "-----Saving Models-----\n",
            "episode 38 score 74.0 avg score 47.6 time_steps 1856 learning_steps 92\n",
            "-----Saving Models-----\n",
            "episode 39 score 98.0 avg score 48.9 time_steps 1954 learning_steps 97\n",
            "-----Saving Models-----\n",
            "episode 40 score 63.0 avg score 49.2 time_steps 2017 learning_steps 100\n",
            "-----Saving Models-----\n",
            "episode 41 score 57.0 avg score 49.4 time_steps 2074 learning_steps 103\n",
            "-----Saving Models-----\n",
            "episode 42 score 66.0 avg score 49.8 time_steps 2140 learning_steps 107\n",
            "-----Saving Models-----\n",
            "episode 43 score 79.0 avg score 50.4 time_steps 2219 learning_steps 110\n",
            "-----Saving Models-----\n",
            "episode 44 score 101.0 avg score 51.6 time_steps 2320 learning_steps 116\n",
            "-----Saving Models-----\n",
            "episode 45 score 151.0 avg score 53.7 time_steps 2471 learning_steps 123\n",
            "-----Saving Models-----\n",
            "episode 46 score 83.0 avg score 54.3 time_steps 2554 learning_steps 127\n",
            "-----Saving Models-----\n",
            "episode 47 score 107.0 avg score 55.4 time_steps 2661 learning_steps 133\n",
            "-----Saving Models-----\n",
            "episode 48 score 94.0 avg score 56.2 time_steps 2755 learning_steps 137\n",
            "-----Saving Models-----\n",
            "episode 49 score 106.0 avg score 57.2 time_steps 2861 learning_steps 143\n",
            "-----Saving Models-----\n",
            "episode 50 score 195.0 avg score 59.9 time_steps 3056 learning_steps 152\n",
            "-----Saving Models-----\n",
            "episode 51 score 170.0 avg score 62.0 time_steps 3226 learning_steps 161\n",
            "-----Saving Models-----\n",
            "episode 52 score 113.0 avg score 63.0 time_steps 3339 learning_steps 166\n",
            "episode 53 score 56.0 avg score 62.9 time_steps 3395 learning_steps 169\n",
            "-----Saving Models-----\n",
            "episode 54 score 75.0 avg score 63.1 time_steps 3470 learning_steps 173\n",
            "-----Saving Models-----\n",
            "episode 55 score 180.0 avg score 65.2 time_steps 3650 learning_steps 182\n",
            "-----Saving Models-----\n",
            "episode 56 score 147.0 avg score 66.6 time_steps 3797 learning_steps 189\n",
            "-----Saving Models-----\n",
            "episode 57 score 200.0 avg score 68.9 time_steps 3997 learning_steps 199\n",
            "-----Saving Models-----\n",
            "episode 58 score 86.0 avg score 69.2 time_steps 4083 learning_steps 204\n",
            "-----Saving Models-----\n",
            "episode 59 score 161.0 avg score 70.7 time_steps 4244 learning_steps 212\n",
            "-----Saving Models-----\n",
            "episode 60 score 200.0 avg score 72.9 time_steps 4444 learning_steps 222\n",
            "-----Saving Models-----\n",
            "episode 61 score 200.0 avg score 74.9 time_steps 4644 learning_steps 232\n",
            "-----Saving Models-----\n",
            "episode 62 score 200.0 avg score 76.9 time_steps 4844 learning_steps 242\n",
            "-----Saving Models-----\n",
            "episode 63 score 200.0 avg score 78.8 time_steps 5044 learning_steps 252\n",
            "-----Saving Models-----\n",
            "episode 64 score 200.0 avg score 80.7 time_steps 5244 learning_steps 262\n",
            "-----Saving Models-----\n",
            "episode 65 score 200.0 avg score 82.5 time_steps 5444 learning_steps 272\n",
            "-----Saving Models-----\n",
            "episode 66 score 154.0 avg score 83.6 time_steps 5598 learning_steps 279\n",
            "-----Saving Models-----\n",
            "episode 67 score 200.0 avg score 85.3 time_steps 5798 learning_steps 289\n",
            "-----Saving Models-----\n",
            "episode 68 score 148.0 avg score 86.2 time_steps 5946 learning_steps 297\n",
            "-----Saving Models-----\n",
            "episode 69 score 109.0 avg score 86.5 time_steps 6055 learning_steps 302\n",
            "-----Saving Models-----\n",
            "episode 70 score 121.0 avg score 87.0 time_steps 6176 learning_steps 308\n",
            "-----Saving Models-----\n",
            "episode 71 score 182.0 avg score 88.3 time_steps 6358 learning_steps 317\n",
            "-----Saving Models-----\n",
            "episode 72 score 200.0 avg score 89.8 time_steps 6558 learning_steps 327\n",
            "-----Saving Models-----\n",
            "episode 73 score 200.0 avg score 91.3 time_steps 6758 learning_steps 337\n",
            "-----Saving Models-----\n",
            "episode 74 score 200.0 avg score 92.8 time_steps 6958 learning_steps 347\n",
            "-----Saving Models-----\n",
            "episode 75 score 200.0 avg score 94.2 time_steps 7158 learning_steps 357\n",
            "-----Saving Models-----\n",
            "episode 76 score 200.0 avg score 95.6 time_steps 7358 learning_steps 367\n",
            "-----Saving Models-----\n",
            "episode 77 score 200.0 avg score 96.9 time_steps 7558 learning_steps 377\n",
            "-----Saving Models-----\n",
            "episode 78 score 117.0 avg score 97.2 time_steps 7675 learning_steps 383\n",
            "-----Saving Models-----\n",
            "episode 79 score 175.0 avg score 98.1 time_steps 7850 learning_steps 392\n",
            "-----Saving Models-----\n",
            "episode 80 score 200.0 avg score 99.4 time_steps 8050 learning_steps 402\n",
            "-----Saving Models-----\n",
            "episode 81 score 164.0 avg score 100.2 time_steps 8214 learning_steps 410\n",
            "-----Saving Models-----\n",
            "episode 82 score 200.0 avg score 101.4 time_steps 8414 learning_steps 420\n",
            "-----Saving Models-----\n",
            "episode 83 score 200.0 avg score 102.5 time_steps 8614 learning_steps 430\n",
            "-----Saving Models-----\n",
            "episode 84 score 200.0 avg score 103.7 time_steps 8814 learning_steps 440\n",
            "-----Saving Models-----\n",
            "episode 85 score 200.0 avg score 104.8 time_steps 9014 learning_steps 450\n",
            "-----Saving Models-----\n",
            "episode 86 score 200.0 avg score 105.9 time_steps 9214 learning_steps 460\n",
            "-----Saving Models-----\n",
            "episode 87 score 200.0 avg score 107.0 time_steps 9414 learning_steps 470\n",
            "-----Saving Models-----\n",
            "episode 88 score 200.0 avg score 108.0 time_steps 9614 learning_steps 480\n",
            "-----Saving Models-----\n",
            "episode 89 score 200.0 avg score 109.0 time_steps 9814 learning_steps 490\n",
            "-----Saving Models-----\n",
            "episode 90 score 200.0 avg score 110.0 time_steps 10014 learning_steps 500\n",
            "-----Saving Models-----\n",
            "episode 91 score 200.0 avg score 111.0 time_steps 10214 learning_steps 510\n",
            "-----Saving Models-----\n",
            "episode 92 score 200.0 avg score 112.0 time_steps 10414 learning_steps 520\n",
            "-----Saving Models-----\n",
            "episode 93 score 200.0 avg score 112.9 time_steps 10614 learning_steps 530\n",
            "-----Saving Models-----\n",
            "episode 94 score 200.0 avg score 113.8 time_steps 10814 learning_steps 540\n",
            "-----Saving Models-----\n",
            "episode 95 score 200.0 avg score 114.7 time_steps 11014 learning_steps 550\n",
            "-----Saving Models-----\n",
            "episode 96 score 200.0 avg score 115.6 time_steps 11214 learning_steps 560\n",
            "-----Saving Models-----\n",
            "episode 97 score 200.0 avg score 116.5 time_steps 11414 learning_steps 570\n",
            "-----Saving Models-----\n",
            "episode 98 score 200.0 avg score 117.3 time_steps 11614 learning_steps 580\n",
            "-----Saving Models-----\n",
            "episode 99 score 187.0 avg score 118.0 time_steps 11801 learning_steps 590\n",
            "-----Saving Models-----\n",
            "episode 100 score 141.0 avg score 119.3 time_steps 11942 learning_steps 597\n",
            "-----Saving Models-----\n",
            "episode 101 score 135.0 avg score 120.5 time_steps 12077 learning_steps 603\n",
            "-----Saving Models-----\n",
            "episode 102 score 99.0 avg score 121.3 time_steps 12176 learning_steps 608\n",
            "-----Saving Models-----\n",
            "episode 103 score 185.0 avg score 123.0 time_steps 12361 learning_steps 618\n",
            "-----Saving Models-----\n",
            "episode 104 score 200.0 avg score 124.8 time_steps 12561 learning_steps 628\n",
            "-----Saving Models-----\n",
            "episode 105 score 200.0 avg score 126.6 time_steps 12761 learning_steps 638\n",
            "-----Saving Models-----\n",
            "episode 106 score 200.0 avg score 128.5 time_steps 12961 learning_steps 648\n",
            "-----Saving Models-----\n",
            "episode 107 score 200.0 avg score 130.3 time_steps 13161 learning_steps 658\n",
            "-----Saving Models-----\n",
            "episode 108 score 200.0 avg score 132.2 time_steps 13361 learning_steps 668\n",
            "-----Saving Models-----\n",
            "episode 109 score 200.0 avg score 134.0 time_steps 13561 learning_steps 678\n",
            "-----Saving Models-----\n",
            "episode 110 score 200.0 avg score 135.9 time_steps 13761 learning_steps 688\n",
            "-----Saving Models-----\n",
            "episode 111 score 200.0 avg score 137.3 time_steps 13961 learning_steps 698\n",
            "-----Saving Models-----\n",
            "episode 112 score 200.0 avg score 139.1 time_steps 14161 learning_steps 708\n",
            "-----Saving Models-----\n",
            "episode 113 score 200.0 avg score 141.0 time_steps 14361 learning_steps 718\n",
            "-----Saving Models-----\n",
            "episode 114 score 200.0 avg score 142.8 time_steps 14561 learning_steps 728\n",
            "-----Saving Models-----\n",
            "episode 115 score 200.0 avg score 144.5 time_steps 14761 learning_steps 738\n",
            "-----Saving Models-----\n",
            "episode 116 score 200.0 avg score 146.4 time_steps 14961 learning_steps 748\n",
            "-----Saving Models-----\n",
            "episode 117 score 200.0 avg score 148.1 time_steps 15161 learning_steps 758\n",
            "-----Saving Models-----\n",
            "episode 118 score 200.0 avg score 149.6 time_steps 15361 learning_steps 768\n",
            "-----Saving Models-----\n",
            "episode 119 score 200.0 avg score 150.2 time_steps 15561 learning_steps 778\n",
            "-----Saving Models-----\n",
            "episode 120 score 200.0 avg score 151.2 time_steps 15761 learning_steps 788\n",
            "-----Saving Models-----\n",
            "episode 121 score 200.0 avg score 152.7 time_steps 15961 learning_steps 798\n",
            "-----Saving Models-----\n",
            "episode 122 score 176.0 avg score 153.8 time_steps 16137 learning_steps 806\n",
            "-----Saving Models-----\n",
            "episode 123 score 187.0 avg score 154.6 time_steps 16324 learning_steps 816\n",
            "-----Saving Models-----\n",
            "episode 124 score 71.0 avg score 154.8 time_steps 16395 learning_steps 819\n",
            "-----Saving Models-----\n",
            "episode 125 score 175.0 avg score 155.8 time_steps 16570 learning_steps 828\n",
            "-----Saving Models-----\n",
            "episode 126 score 185.0 avg score 157.0 time_steps 16755 learning_steps 837\n",
            "-----Saving Models-----\n",
            "episode 127 score 183.0 avg score 158.5 time_steps 16938 learning_steps 846\n",
            "-----Saving Models-----\n",
            "episode 128 score 200.0 avg score 159.8 time_steps 17138 learning_steps 856\n",
            "-----Saving Models-----\n",
            "episode 129 score 200.0 avg score 161.3 time_steps 17338 learning_steps 866\n",
            "-----Saving Models-----\n",
            "episode 130 score 147.0 avg score 162.0 time_steps 17485 learning_steps 874\n",
            "-----Saving Models-----\n",
            "episode 131 score 200.0 avg score 163.3 time_steps 17685 learning_steps 884\n",
            "-----Saving Models-----\n",
            "episode 132 score 181.0 avg score 164.5 time_steps 17866 learning_steps 893\n",
            "-----Saving Models-----\n",
            "episode 133 score 200.0 avg score 165.8 time_steps 18066 learning_steps 903\n",
            "-----Saving Models-----\n",
            "episode 134 score 197.0 avg score 167.4 time_steps 18263 learning_steps 913\n",
            "-----Saving Models-----\n",
            "episode 135 score 200.0 avg score 168.7 time_steps 18463 learning_steps 923\n",
            "-----Saving Models-----\n",
            "episode 136 score 200.0 avg score 170.4 time_steps 18663 learning_steps 933\n",
            "-----Saving Models-----\n",
            "episode 137 score 200.0 avg score 170.8 time_steps 18863 learning_steps 943\n",
            "-----Saving Models-----\n",
            "episode 138 score 200.0 avg score 172.1 time_steps 19063 learning_steps 953\n",
            "-----Saving Models-----\n",
            "episode 139 score 200.0 avg score 173.1 time_steps 19263 learning_steps 963\n",
            "-----Saving Models-----\n",
            "episode 140 score 165.0 avg score 174.1 time_steps 19428 learning_steps 971\n",
            "-----Saving Models-----\n",
            "episode 141 score 200.0 avg score 175.5 time_steps 19628 learning_steps 981\n",
            "-----Saving Models-----\n",
            "episode 142 score 173.0 avg score 176.6 time_steps 19801 learning_steps 990\n",
            "-----Saving Models-----\n",
            "episode 143 score 200.0 avg score 177.8 time_steps 20001 learning_steps 1000\n",
            "-----Saving Models-----\n",
            "episode 144 score 200.0 avg score 178.8 time_steps 20201 learning_steps 1010\n",
            "-----Saving Models-----\n",
            "episode 145 score 200.0 avg score 179.3 time_steps 20401 learning_steps 1020\n",
            "-----Saving Models-----\n",
            "episode 146 score 200.0 avg score 180.5 time_steps 20601 learning_steps 1030\n",
            "-----Saving Models-----\n",
            "episode 147 score 200.0 avg score 181.4 time_steps 20801 learning_steps 1040\n",
            "-----Saving Models-----\n",
            "episode 148 score 200.0 avg score 182.5 time_steps 21001 learning_steps 1050\n",
            "-----Saving Models-----\n",
            "episode 149 score 159.0 avg score 183.0 time_steps 21160 learning_steps 1058\n",
            "-----Saving Models-----\n",
            "episode 150 score 200.0 avg score 183.0 time_steps 21360 learning_steps 1068\n",
            "-----Saving Models-----\n",
            "episode 151 score 200.0 avg score 183.3 time_steps 21560 learning_steps 1078\n",
            "-----Saving Models-----\n",
            "episode 152 score 200.0 avg score 184.2 time_steps 21760 learning_steps 1088\n",
            "-----Saving Models-----\n",
            "episode 153 score 200.0 avg score 185.7 time_steps 21960 learning_steps 1098\n",
            "-----Saving Models-----\n",
            "episode 154 score 200.0 avg score 186.9 time_steps 22160 learning_steps 1108\n",
            "-----Saving Models-----\n",
            "episode 155 score 200.0 avg score 187.1 time_steps 22360 learning_steps 1118\n",
            "-----Saving Models-----\n",
            "episode 156 score 200.0 avg score 187.6 time_steps 22560 learning_steps 1128\n",
            "episode 157 score 200.0 avg score 187.6 time_steps 22760 learning_steps 1138\n",
            "-----Saving Models-----\n",
            "episode 158 score 200.0 avg score 188.8 time_steps 22960 learning_steps 1148\n",
            "-----Saving Models-----\n",
            "episode 159 score 188.0 avg score 189.0 time_steps 23148 learning_steps 1157\n",
            "episode 160 score 200.0 avg score 189.0 time_steps 23348 learning_steps 1167\n",
            "episode 161 score 200.0 avg score 189.0 time_steps 23548 learning_steps 1177\n",
            "episode 162 score 200.0 avg score 189.0 time_steps 23748 learning_steps 1187\n",
            "episode 163 score 200.0 avg score 189.0 time_steps 23948 learning_steps 1197\n",
            "episode 164 score 200.0 avg score 189.0 time_steps 24148 learning_steps 1207\n",
            "episode 165 score 200.0 avg score 189.0 time_steps 24348 learning_steps 1217\n",
            "-----Saving Models-----\n",
            "episode 166 score 200.0 avg score 189.5 time_steps 24548 learning_steps 1227\n",
            "episode 167 score 200.0 avg score 189.5 time_steps 24748 learning_steps 1237\n",
            "episode 168 score 131.0 avg score 189.3 time_steps 24879 learning_steps 1243\n",
            "-----Saving Models-----\n",
            "episode 169 score 158.0 avg score 189.8 time_steps 25037 learning_steps 1251\n",
            "-----Saving Models-----\n",
            "episode 170 score 171.0 avg score 190.3 time_steps 25208 learning_steps 1260\n",
            "episode 171 score 110.0 avg score 189.6 time_steps 25318 learning_steps 1265\n",
            "episode 172 score 61.0 avg score 188.2 time_steps 25379 learning_steps 1268\n",
            "episode 173 score 11.0 avg score 186.3 time_steps 25390 learning_steps 1269\n",
            "episode 174 score 10.0 avg score 184.4 time_steps 25400 learning_steps 1270\n",
            "episode 175 score 11.0 avg score 182.5 time_steps 25411 learning_steps 1270\n",
            "episode 176 score 134.0 avg score 181.9 time_steps 25545 learning_steps 1277\n",
            "episode 177 score 200.0 avg score 181.9 time_steps 25745 learning_steps 1287\n",
            "episode 178 score 200.0 avg score 182.7 time_steps 25945 learning_steps 1297\n",
            "episode 179 score 132.0 avg score 182.3 time_steps 26077 learning_steps 1303\n",
            "episode 180 score 123.0 avg score 181.5 time_steps 26200 learning_steps 1310\n",
            "episode 181 score 114.0 avg score 181.0 time_steps 26314 learning_steps 1315\n",
            "episode 182 score 102.0 avg score 180.0 time_steps 26416 learning_steps 1320\n",
            "episode 183 score 77.0 avg score 178.8 time_steps 26493 learning_steps 1324\n",
            "episode 184 score 88.0 avg score 177.7 time_steps 26581 learning_steps 1329\n",
            "episode 185 score 75.0 avg score 176.4 time_steps 26656 learning_steps 1332\n",
            "episode 186 score 68.0 avg score 175.1 time_steps 26724 learning_steps 1336\n",
            "episode 187 score 50.0 avg score 173.6 time_steps 26774 learning_steps 1338\n",
            "episode 188 score 58.0 avg score 172.2 time_steps 26832 learning_steps 1341\n",
            "episode 189 score 79.0 avg score 171.0 time_steps 26911 learning_steps 1345\n",
            "episode 190 score 86.0 avg score 169.8 time_steps 26997 learning_steps 1349\n",
            "episode 191 score 85.0 avg score 168.7 time_steps 27082 learning_steps 1354\n",
            "episode 192 score 108.0 avg score 167.8 time_steps 27190 learning_steps 1359\n",
            "episode 193 score 99.0 avg score 166.8 time_steps 27289 learning_steps 1364\n",
            "episode 194 score 104.0 avg score 165.8 time_steps 27393 learning_steps 1369\n",
            "episode 195 score 75.0 avg score 164.5 time_steps 27468 learning_steps 1373\n",
            "episode 196 score 90.0 avg score 163.4 time_steps 27558 learning_steps 1377\n",
            "episode 197 score 109.0 avg score 162.5 time_steps 27667 learning_steps 1383\n",
            "episode 198 score 200.0 avg score 162.5 time_steps 27867 learning_steps 1393\n",
            "episode 199 score 200.0 avg score 162.7 time_steps 28067 learning_steps 1403\n",
            "episode 200 score 200.0 avg score 163.2 time_steps 28267 learning_steps 1413\n",
            "episode 201 score 200.0 avg score 163.9 time_steps 28467 learning_steps 1423\n",
            "episode 202 score 200.0 avg score 164.9 time_steps 28667 learning_steps 1433\n",
            "episode 203 score 200.0 avg score 165.1 time_steps 28867 learning_steps 1443\n",
            "episode 204 score 200.0 avg score 165.1 time_steps 29067 learning_steps 1453\n",
            "episode 205 score 200.0 avg score 165.1 time_steps 29267 learning_steps 1463\n",
            "episode 206 score 200.0 avg score 165.1 time_steps 29467 learning_steps 1473\n",
            "episode 207 score 200.0 avg score 165.1 time_steps 29667 learning_steps 1483\n",
            "episode 208 score 200.0 avg score 165.1 time_steps 29867 learning_steps 1493\n",
            "episode 209 score 200.0 avg score 165.1 time_steps 30067 learning_steps 1503\n",
            "episode 210 score 200.0 avg score 165.1 time_steps 30267 learning_steps 1513\n",
            "episode 211 score 200.0 avg score 165.1 time_steps 30467 learning_steps 1523\n",
            "episode 212 score 200.0 avg score 165.1 time_steps 30667 learning_steps 1533\n",
            "episode 213 score 200.0 avg score 165.1 time_steps 30867 learning_steps 1543\n",
            "episode 214 score 200.0 avg score 165.1 time_steps 31067 learning_steps 1553\n",
            "episode 215 score 200.0 avg score 165.1 time_steps 31267 learning_steps 1563\n",
            "episode 216 score 200.0 avg score 165.1 time_steps 31467 learning_steps 1573\n",
            "episode 217 score 200.0 avg score 165.1 time_steps 31667 learning_steps 1583\n",
            "episode 218 score 200.0 avg score 165.1 time_steps 31867 learning_steps 1593\n",
            "episode 219 score 200.0 avg score 165.1 time_steps 32067 learning_steps 1603\n",
            "episode 220 score 200.0 avg score 165.1 time_steps 32267 learning_steps 1613\n",
            "episode 221 score 184.0 avg score 164.9 time_steps 32451 learning_steps 1622\n",
            "episode 222 score 200.0 avg score 165.1 time_steps 32651 learning_steps 1632\n",
            "episode 223 score 200.0 avg score 165.3 time_steps 32851 learning_steps 1642\n",
            "episode 224 score 200.0 avg score 166.6 time_steps 33051 learning_steps 1652\n",
            "episode 225 score 191.0 avg score 166.7 time_steps 33242 learning_steps 1662\n",
            "episode 226 score 200.0 avg score 166.9 time_steps 33442 learning_steps 1672\n",
            "episode 227 score 186.0 avg score 166.9 time_steps 33628 learning_steps 1681\n",
            "episode 228 score 192.0 avg score 166.8 time_steps 33820 learning_steps 1691\n",
            "episode 229 score 200.0 avg score 166.8 time_steps 34020 learning_steps 1701\n",
            "episode 230 score 200.0 avg score 167.3 time_steps 34220 learning_steps 1711\n",
            "episode 231 score 200.0 avg score 167.3 time_steps 34420 learning_steps 1721\n",
            "episode 232 score 200.0 avg score 167.5 time_steps 34620 learning_steps 1731\n",
            "episode 233 score 200.0 avg score 167.5 time_steps 34820 learning_steps 1741\n",
            "episode 234 score 200.0 avg score 167.6 time_steps 35020 learning_steps 1751\n",
            "episode 235 score 200.0 avg score 167.6 time_steps 35220 learning_steps 1761\n",
            "episode 236 score 200.0 avg score 167.6 time_steps 35420 learning_steps 1771\n",
            "episode 237 score 200.0 avg score 167.6 time_steps 35620 learning_steps 1781\n",
            "episode 238 score 200.0 avg score 167.6 time_steps 35820 learning_steps 1791\n",
            "episode 239 score 200.0 avg score 167.6 time_steps 36020 learning_steps 1801\n",
            "episode 240 score 200.0 avg score 167.9 time_steps 36220 learning_steps 1811\n",
            "episode 241 score 200.0 avg score 167.9 time_steps 36420 learning_steps 1821\n",
            "episode 242 score 200.0 avg score 168.2 time_steps 36620 learning_steps 1831\n",
            "episode 243 score 200.0 avg score 168.2 time_steps 36820 learning_steps 1841\n",
            "episode 244 score 200.0 avg score 168.2 time_steps 37020 learning_steps 1851\n",
            "episode 245 score 200.0 avg score 168.2 time_steps 37220 learning_steps 1861\n",
            "episode 246 score 200.0 avg score 168.2 time_steps 37420 learning_steps 1871\n",
            "episode 247 score 200.0 avg score 168.2 time_steps 37620 learning_steps 1881\n",
            "episode 248 score 200.0 avg score 168.2 time_steps 37820 learning_steps 1891\n",
            "episode 249 score 200.0 avg score 168.6 time_steps 38020 learning_steps 1901\n",
            "episode 250 score 200.0 avg score 168.6 time_steps 38220 learning_steps 1911\n",
            "episode 251 score 200.0 avg score 168.6 time_steps 38420 learning_steps 1921\n",
            "episode 252 score 200.0 avg score 168.6 time_steps 38620 learning_steps 1931\n",
            "episode 253 score 200.0 avg score 168.6 time_steps 38820 learning_steps 1941\n",
            "episode 254 score 200.0 avg score 168.6 time_steps 39020 learning_steps 1951\n",
            "episode 255 score 200.0 avg score 168.6 time_steps 39220 learning_steps 1961\n",
            "episode 256 score 200.0 avg score 168.6 time_steps 39420 learning_steps 1971\n",
            "episode 257 score 200.0 avg score 168.6 time_steps 39620 learning_steps 1981\n",
            "episode 258 score 200.0 avg score 168.6 time_steps 39820 learning_steps 1991\n",
            "episode 259 score 200.0 avg score 168.7 time_steps 40020 learning_steps 2001\n",
            "episode 260 score 200.0 avg score 168.7 time_steps 40220 learning_steps 2011\n",
            "episode 261 score 200.0 avg score 168.7 time_steps 40420 learning_steps 2021\n",
            "episode 262 score 200.0 avg score 168.7 time_steps 40620 learning_steps 2031\n",
            "episode 263 score 200.0 avg score 168.7 time_steps 40820 learning_steps 2041\n",
            "episode 264 score 200.0 avg score 168.7 time_steps 41020 learning_steps 2051\n",
            "episode 265 score 200.0 avg score 168.7 time_steps 41220 learning_steps 2061\n",
            "episode 266 score 200.0 avg score 168.7 time_steps 41420 learning_steps 2071\n",
            "episode 267 score 200.0 avg score 168.7 time_steps 41620 learning_steps 2081\n",
            "episode 268 score 200.0 avg score 169.4 time_steps 41820 learning_steps 2091\n",
            "episode 269 score 200.0 avg score 169.8 time_steps 42020 learning_steps 2101\n",
            "episode 270 score 200.0 avg score 170.1 time_steps 42220 learning_steps 2111\n",
            "episode 271 score 200.0 avg score 171.0 time_steps 42420 learning_steps 2121\n",
            "episode 272 score 200.0 avg score 172.4 time_steps 42620 learning_steps 2131\n",
            "episode 273 score 200.0 avg score 174.3 time_steps 42820 learning_steps 2141\n",
            "episode 274 score 200.0 avg score 176.2 time_steps 43020 learning_steps 2151\n",
            "episode 275 score 200.0 avg score 178.1 time_steps 43220 learning_steps 2161\n",
            "episode 276 score 200.0 avg score 178.8 time_steps 43420 learning_steps 2171\n",
            "episode 277 score 200.0 avg score 178.8 time_steps 43620 learning_steps 2181\n",
            "episode 278 score 200.0 avg score 178.8 time_steps 43820 learning_steps 2191\n",
            "episode 279 score 200.0 avg score 179.4 time_steps 44020 learning_steps 2201\n",
            "episode 280 score 200.0 avg score 180.2 time_steps 44220 learning_steps 2211\n",
            "episode 281 score 200.0 avg score 181.1 time_steps 44420 learning_steps 2221\n",
            "episode 282 score 200.0 avg score 182.0 time_steps 44620 learning_steps 2231\n",
            "episode 283 score 200.0 avg score 183.3 time_steps 44820 learning_steps 2241\n",
            "episode 284 score 200.0 avg score 184.4 time_steps 45020 learning_steps 2251\n",
            "episode 285 score 200.0 avg score 185.6 time_steps 45220 learning_steps 2261\n",
            "episode 286 score 200.0 avg score 187.0 time_steps 45420 learning_steps 2271\n",
            "episode 287 score 200.0 avg score 188.5 time_steps 45620 learning_steps 2281\n",
            "episode 288 score 200.0 avg score 189.9 time_steps 45820 learning_steps 2291\n",
            "-----Saving Models-----\n",
            "episode 289 score 200.0 avg score 191.1 time_steps 46020 learning_steps 2301\n",
            "-----Saving Models-----\n",
            "episode 290 score 200.0 avg score 192.2 time_steps 46220 learning_steps 2311\n",
            "-----Saving Models-----\n",
            "episode 291 score 200.0 avg score 193.4 time_steps 46420 learning_steps 2321\n",
            "-----Saving Models-----\n",
            "episode 292 score 200.0 avg score 194.3 time_steps 46620 learning_steps 2331\n",
            "-----Saving Models-----\n",
            "episode 293 score 200.0 avg score 195.3 time_steps 46820 learning_steps 2341\n",
            "-----Saving Models-----\n",
            "episode 294 score 200.0 avg score 196.3 time_steps 47020 learning_steps 2351\n",
            "-----Saving Models-----\n",
            "episode 295 score 200.0 avg score 197.5 time_steps 47220 learning_steps 2361\n",
            "-----Saving Models-----\n",
            "episode 296 score 200.0 avg score 198.6 time_steps 47420 learning_steps 2371\n",
            "-----Saving Models-----\n",
            "episode 297 score 200.0 avg score 199.5 time_steps 47620 learning_steps 2381\n",
            "episode 298 score 200.0 avg score 199.5 time_steps 47820 learning_steps 2391\n",
            "episode 299 score 200.0 avg score 199.5 time_steps 48020 learning_steps 2401\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZhIKxj_U7LB"
      },
      "source": [
        "%%bash\n",
        "# Install additional packages for visualization\n",
        "sudo apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "pip install git+https://github.com/tensorflow/docs > /dev/null 2>&1"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgtsBlWT3T7a"
      },
      "source": [
        "from IPython import display as ipythondisplay\n",
        "from PIL import Image\n",
        "\n",
        "def render_episode(env, model, max_steps): \n",
        "\n",
        "  screen = env.render(mode='rgb_array')\n",
        "  im = Image.fromarray(screen)\n",
        "\n",
        "  images = [im]\n",
        "  \n",
        "  state = env.reset()\n",
        "  for i in range(1, max_steps + 1):\n",
        "    prob, action_, val = model.choose_action(state)\n",
        "\n",
        "    state, _, done, _ = env.step(action)\n",
        "\n",
        "    # Render screen every 10 steps\n",
        "    if i % 10 == 0:\n",
        "      screen = env.render(mode='rgb_array')\n",
        "      images.append(Image.fromarray(screen))\n",
        "  \n",
        "    if done:\n",
        "      break\n",
        "  \n",
        "  return images"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhNRp-VK6JOU"
      },
      "source": [
        "import pyvirtualdisplay\n",
        "\n",
        "\n",
        "_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
        "                                    size=(1400, 900))\n",
        "_ = _display.start()\n",
        "\n",
        "# Save GIF image\n",
        "images = render_episode(env, agent, 100)\n",
        "image_file = 'cartpole-v0.gif'\n",
        "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
        "images[0].save(\n",
        "    image_file, save_all=True, append_images=images[1:], loop=0, duration=1)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "CbSOlswO7RSW",
        "outputId": "8b55444a-2e5d-4ac4-c52f-1ab0ebee4aa1"
      },
      "source": [
        "import tensorflow_docs.vis.embed as embed\n",
        "embed.embed_file(image_file)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"data:image/gif;base64,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\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHanmKcUevTU"
      },
      "source": [
        ""
      ]
    }
  ]
}